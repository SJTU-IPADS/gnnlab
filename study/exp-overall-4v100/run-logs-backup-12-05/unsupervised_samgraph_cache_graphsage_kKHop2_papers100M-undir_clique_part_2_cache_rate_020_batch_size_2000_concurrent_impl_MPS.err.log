[2022-12-05 16:38:22.990201: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:92] mmap allocating space 12.03 GB
[2022-12-05 16:38:22.990300: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:97] mmap allocating space 12.03 GB done
[2022-12-05 16:38:28.164326: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:92] mmap allocating space 52.96 GB
[2022-12-05 16:38:28.164435: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:97] mmap allocating space 52.96 GB done
[2022-12-05 16:38:28.164525: W /samgraph/samgraph/common/cpu/cpu_device.cc:39] WORKER[0] alloc host memory 12.03 GB
[2022-12-05 16:38:28.968376: E /samgraph/samgraph/common/engine.cc:272] Train set size 7.64 MB
[2022-12-05 16:38:28.969171: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:92] mmap allocating space 68.50 GB
[2022-12-05 16:38:28.969216: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:97] mmap allocating space 68.50 GB done
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
[2022-12-05 16:38:29.178945: W /samgraph/samgraph/common/dist/dist_engine.cc:662] Trainer[1] initializing...
[2022-12-05 16:38:29.180036: W /samgraph/samgraph/common/dist/dist_engine.cc:662] Trainer[2] initializing...
[2022-12-05 16:38:29.195626: W /samgraph/samgraph/common/dist/dist_engine.cc:662] Trainer[0] initializing...
[2022-12-05 16:38:30.692252: W /samgraph/samgraph/common/dist/dist_engine.cc:666] Trainer[0] pin memory queue...
[2022-12-05 16:38:30.701521: W /samgraph/samgraph/common/dist/dist_engine.cc:666] Trainer[1] pin memory queue...
[2022-12-05 16:38:30.710781: W /samgraph/samgraph/common/dist/dist_engine.cc:666] Trainer[2] pin memory queue...
[2022-12-05 16:38:54.899039: E /samgraph/samgraph/common/dist/dist_engine.cc:85] Running on V100
[2022-12-05 16:38:54.899617: W /samgraph/samgraph/common/dist/dist_engine.cc:701] Trainer[0] register host memory...
[2022-12-05 16:38:54.903253: E /samgraph/samgraph/common/dist/dist_engine.cc:85] Running on V100
[2022-12-05 16:38:54.903626: W /samgraph/samgraph/common/dist/dist_engine.cc:701] Trainer[1] register host memory...
[2022-12-05 16:38:54.923830: E /samgraph/samgraph/common/dist/dist_engine.cc:85] Running on V100
[2022-12-05 16:38:54.923933: W /samgraph/samgraph/common/dist/dist_engine.cc:701] Trainer[2] register host memory...
[2022-12-05 16:38:55.143954: E /samgraph/samgraph/common/dist/dist_engine.cc:85] Running on V100
[2022-12-05 16:38:55.149203: W /samgraph/samgraph/common/cuda/cuda_device.cc:49] GPU[3] alloc cuda memory 423.66 MB
[2022-12-05 16:38:55.264675: W /samgraph/samgraph/common/cuda/cuda_device.cc:49] GPU[3] alloc cuda memory 12.03 GB
[2022-12-05 16:38:58.271894: W /samgraph/samgraph/common/cuda/cuda_device.cc:49] GPU[3] alloc cuda memory 64.00 MB
[2022-12-05 16:38:58.277381: E /samgraph/samgraph/common/dist/pre_sampler.cc:41] Dist Presampler making shuffler...
[2022-12-05 16:38:58.282598: E /samgraph/samgraph/common/dist/pre_sampler.cc:44] Dist Presampler making shuffler...Done
[2022-12-05 16:38:58.282645: W /samgraph/samgraph/common/cpu/cpu_device.cc:39] WORKER[0] alloc host memory 847.32 MB
[2022-12-05 16:38:58.583010: E /samgraph/samgraph/common/dist/pre_sampler.cc:85] Dist Presampler doing presample epoch 0
[2022-12-05 16:39:08. 68877: E /samgraph/samgraph/common/dist/pre_sampler.cc:129] presample spend 4.57509 on sample, 0.365342 on copy, 4.53365 on count
[2022-12-05 16:39:08. 69003: E /samgraph/samgraph/common/dist/pre_sampler.cc:85] Dist Presampler doing presample epoch 1
[2022-12-05 16:39:17.558363: E /samgraph/samgraph/common/dist/pre_sampler.cc:129] presample spend 9.13925 on sample, 0.728564 on copy, 9.08389 on count
[2022-12-05 16:39:17.558497: E /samgraph/samgraph/common/dist/pre_sampler.cc:136] max_num_inputs = 1173848, min_num_inputs = 1135872
[2022-12-05 16:39:17.897501: E /samgraph/samgraph/common/dist/pre_sampler.cc:148] presample spend 0.338921 on sort freq.
[2022-12-05 16:39:18.233331: E /samgraph/samgraph/common/dist/dist_engine.cc:524] pre sample done, delete it
[[[2022-12-05 16:39:182022-12-05 16:39:18.2022-12-05 16:39:18.261846261843.: 261844: W: W W /samgraph/samgraph/common/dist/dist_engine.cc /samgraph/samgraph/common/dist/dist_engine.cc:/samgraph/samgraph/common/dist/dist_engine.cc:711:711] 711] Trainer[2] building cache...] Trainer[0] building cache...
Trainer[1] building cache...

[2022-12-05 16:39:18.263718: E /samgraph/3rdparty/collcachelib/coll_cache_lib/run_config.cc:134] using concurrent impl MPS
[2022-12-05 16:39:18.263893: E /samgraph/3rdparty/collcachelib/coll_cache_lib/run_config.cc:134] using concurrent impl MPS
[2022-12-05 16:39:18.264063: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] build symm link desc with 3X Tesla V100-SXM2-16GB out of 4
[2022-12-05 16:39:18.264113: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:197] remote time is 8.68421
[2022-12-05 16:39:18.264142: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:198] cpu time is 30
[2022-12-05 16:39:18.264517: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] build symm link desc with 3X Tesla V100-SXM2-16GB out of 4
[2022-12-05 16:39:18.264617: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:197] remote time is 8.68421
[2022-12-05 16:39:18.264695: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:198] cpu time is 30
[2022-12-05 16:39:18.270300: E /samgraph/3rdparty/collcachelib/coll_cache_lib/run_config.cc:134] using concurrent impl MPS
[2022-12-05 16:39:18.270924: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] build symm link desc with 3X Tesla V100-SXM2-16GB out of 4
[2022-12-05 16:39:18.270973: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:197] remote time is 8.68421
[2022-12-05 16:39:18.271002: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:198] cpu time is 30
[2022-12-05 16:39:35.758671: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:115] solver created. now build & solve
[2022-12-05 16:39:35.806499: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:120] solver built. now solve
block 0 storage is 00000001
	access is	0	0	0	
block 1 storage is 00000010
	access is	1	1	1	
block 2 storage is 00000100
	access is	2	2	2	
block 3 storage is 00000000
	access is	3	3	3	
[2022-12-05 16:39:36.488371: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:124] solver solved
[2022-12-05 16:39:36.488505: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:187] 0 solved master
[2022-12-05 16:39:36.488599: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:194] 0 solved
[2022-12-05 16:39:36.488629: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:197] worker 0 thread 0 initing device 0
[2022-12-05 16:39:36.489893: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1762] Building Coll Cache with ... num gpu device is 3
[[2022-12-05 16:39:362022-12-05 16:39:36..508582508585: : EE  /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc/samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc::194194] ] 2 solved
1 solved
[2022-12-05 16:39:36.[5086622022-12-05 16:39:36: .E508667 : /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.ccE: 197/samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc] :worker 2 thread 2 initing device 2197
] worker 1 thread 1 initing device 1
[2022-12-05 16:39:36.509997: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1762] Building Coll Cache with ... num gpu device is 3
[2022-12-05 16:39:36.510246: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1762] Building Coll Cache with ... num gpu device is 3
[2022-12-05 16:39:36.582306: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cpu/cpu_device.cc:39] WORKER[0] alloc host memory 84.73 MB
[[[2022-12-05 16:39:372022-12-05 16:39:372022-12-05 16:39:37...616642616643616647: : : EEE   /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu/samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu/samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:::187118711871] ] ] Device 1 init p2p of link 2Device 0 init p2p of link 1Device 2 init p2p of link 0


[2022-12-05 16:39:37.627694: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1871] Device 1 init p2p of link 0
[2022-12-05 16:39:37.627846: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1871] Device 0 init p2p of link 2
[2022-12-05 16:39:37.628679: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1871] Device 2 init p2p of link 1
[2022-12-05 16:39:37.639112: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1900] Asymm Coll cache (policy: clique_part) | local 22211991 / 111059956 nodes ( 20.00 %~20.00 %) | remote 44423982 / 111059956 nodes ( 40.00 %) | cpu 44423983 / 111059956 nodes ( 40.00 %) | 10.60 GB | 1.12882 secs 
[2022-12-05 16:39:37.639682: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1900] Asymm Coll cache (policy: clique_part) | local 22211991 / 111059956 nodes ( 20.00 %~20.00 %) | remote 44423982 / 111059956 nodes ( 40.00 %) | cpu 44423983 / 111059956 nodes ( 40.00 %) | 10.60 GB | 1.14975 secs 
[2022-12-05 16:39:37.640289: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:[19002022-12-05 16:39:37] .640299Asymm Coll cache (policy: clique_part) | local 22211991 / 111059956 nodes ( 20.00 %~20.00 %) | remote 44423982 / 111059956 nodes ( 40.00 %) | cpu 44423983 / 111059956 nodes ( 40.00 %) | 10.60 GB | 1.13025 secs : 
W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2004] before create ctx, mem is 12.36 GB
[2022-12-05 16:39:39.   928: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2010] after create ctx, mem is 12.85 GB
[2022-12-05 16:39:39.  1039: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2017] after create stream, mem is 12.85 GB
[2022-12-05 16:39:39.  1284: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2004] before create ctx, mem is 12.85 GB
[2022-12-05 16:39:40.107154: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2010] after create ctx, mem is 13.34 GB
[2022-12-05 16:39:40.107265: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2017] after create stream, mem is 13.34 GB
[2022-12-05 16:39:40.107607: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2004] before create ctx, mem is 13.34 GB
[2022-12-05 16:39:41.417381: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2010] after create ctx, mem is 13.98 GB
[2022-12-05 16:39:41.417590: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2017] after create stream, mem is 13.98 GB
[2022-12-05 16:39:41.417914: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2004] before create ctx, mem is 13.98 GB
[2022-12-05 16:39:42.882303: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2010] after create ctx, mem is 14.35 GB
[2022-12-05 16:39:42.882497: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2017] after create stream, mem is 14.35 GB
[2022-12-05 16:39:50.362677: W /samgraph/samgraph/common/cuda/cuda_device.cc:49] GPU[0] alloc cuda memory 619.38 MB
[2022-12-05 16:39:50.363703: W /samgraph/samgraph/common/cuda/cuda_device.cc:49] GPU[0] alloc cuda memory 12.10 MB
[2022-12-05 16:39:50.364511: W /samgraph/samgraph/common/cuda/cuda_device.cc:49] GPU[0] alloc cuda memory 12.10 MB
/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn("is_namedtuple is deprecated, please use the python checks instead")
Process Process-2:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 33, in decorated_function
    raise exception.__class__(trace)
RuntimeError: Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 21, in _queue_result
    res = func(*args, **kwargs)
  File "../../example/samgraph/multi_gpu/unsupervised/train_graphsage.py", line 349, in run_train
    batch_pred = model(blocks, batch_input)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1009, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 970, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "../../example/samgraph/multi_gpu/unsupervised/train_graphsage.py", line 47, in forward
    h = layer(block, h)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/nn/pytorch/conv/sageconv.py", line 238, in forward
    h_neigh = self.fc_neigh(h_neigh)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`

/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn("is_namedtuple is deprecated, please use the python checks instead")
Process Process-3:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 33, in decorated_function
    raise exception.__class__(trace)
RuntimeError: Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 21, in _queue_result
    res = func(*args, **kwargs)
  File "../../example/samgraph/multi_gpu/unsupervised/train_graphsage.py", line 349, in run_train
    batch_pred = model(blocks, batch_input)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1009, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 970, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "../../example/samgraph/multi_gpu/unsupervised/train_graphsage.py", line 47, in forward
    h = layer(block, h)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/nn/pytorch/conv/sageconv.py", line 238, in forward
    h_neigh = self.fc_neigh(h_neigh)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`

/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn("is_namedtuple is deprecated, please use the python checks instead")
Process Process-4:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 33, in decorated_function
    raise exception.__class__(trace)
RuntimeError: Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 21, in _queue_result
    res = func(*args, **kwargs)
  File "../../example/samgraph/multi_gpu/unsupervised/train_graphsage.py", line 349, in run_train
    batch_pred = model(blocks, batch_input)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1009, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 970, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "../../example/samgraph/multi_gpu/unsupervised/train_graphsage.py", line 47, in forward
    h = layer(block, h)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/nn/pytorch/conv/sageconv.py", line 238, in forward
    h_neigh = self.fc_neigh(h_neigh)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`

[2022-12-05 16:39:54.770538: E /samgraph/samgraph/common/operation.cc:567] detect a terminated child 124543, status is 1
