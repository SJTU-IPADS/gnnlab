[2022-12-08 19:20:39.757965: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:92] mmap allocating space 12.87 GB
[2022-12-08 19:20:39.758076: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:97] mmap allocating space 12.87 GB done
[2022-12-08 19:20:46.763993: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:92] mmap allocating space 48.00 GB
[2022-12-08 19:20:46.764085: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:97] mmap allocating space 48.00 GB done
[2022-12-08 19:20:46.764122: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:92] mmap allocating space 1.82 GB
[2022-12-08 19:20:46.764154: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:97] mmap allocating space 1.82 GB done
[2022-12-08 19:20:46.764205: W /samgraph/samgraph/common/cpu/cpu_device.cc:39] WORKER[0] alloc host memory 12.87 GB
[2022-12-08 19:20:47.904369: E /samgraph/samgraph/common/engine.cc:272] Train set size 3.82 MB
[2022-12-08 19:20:47.905208: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:92] mmap allocating space 293.56 GB
[2022-12-08 19:20:47.905244: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:97] mmap allocating space 293.56 GB done
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
[2022-12-08 19:20:48.556991: W /samgraph/samgraph/common/dist/dist_engine.cc:662] Trainer[0] initializing...
[2022-12-08 19:20:48.653477: W /samgraph/samgraph/common/dist/dist_engine.cc:662] Trainer[1] initializing...
[2022-12-08 19:20:48.729952: W /samgraph/samgraph/common/dist/dist_engine.cc:662] Trainer[2] initializing...
[2022-12-08 19:20:48.901801: W /samgraph/samgraph/common/dist/dist_engine.cc:662] Trainer[3] initializing...
[2022-12-08 19:20:48.950050: W /samgraph/samgraph/common/dist/dist_engine.cc:662] Trainer[5] initializing...
[2022-12-08 19:20:49. 19672: W /samgraph/samgraph/common/dist/dist_engine.cc:662] Trainer[4] initializing...
[2022-12-08 19:20:50.755107: W /samgraph/samgraph/common/dist/dist_engine.cc:666] Trainer[0] pin memory queue...
[2022-12-08 19:20:50.886084: W /samgraph/samgraph/common/dist/dist_engine.cc:666] Trainer[5] pin memory queue...
[2022-12-08 19:20:50.891979: W /samgraph/samgraph/common/dist/dist_engine.cc:666] Trainer[3] pin memory queue...
[2022-12-08 19:20:50.901691: W /samgraph/samgraph/common/dist/dist_engine.cc:666] Trainer[4] pin memory queue...
[2022-12-08 19:20:50.902534: W /samgraph/samgraph/common/dist/dist_engine.cc:666] Trainer[1] pin memory queue...
[2022-12-08 19:20:50.917415: W /samgraph/samgraph/common/dist/dist_engine.cc:666] Trainer[2] pin memory queue...
[2022-12-08 19:23:29. 35511: E /samgraph/samgraph/common/dist/dist_engine.cc:85] Running on V100
[2022-12-08 19:23:29. 35963: E /samgraph/samgraph/common/dist/dist_engine.cc:85] Running on V100
[2022-12-08 19:23:29. 36416: E /samgraph/samgraph/common/dist/dist_engine.cc:85] Running on V100
[2022-12-08 19:23:29. 36548: E /samgraph/samgraph/common/dist/dist_engine.cc:85] Running on V100
[2022-12-08 19:23:29. 36676: W /samgraph/samgraph/common/dist/dist_engine.cc:701] Trainer[2] register host memory...
[2022-12-08 19:23:29. 37209: W /samgraph/samgraph/common/dist/dist_engine.cc:701] Trainer[0] register host memory...
[2022-12-08 19:23:29. 37827: W /samgraph/samgraph/common/dist/dist_engine.cc:701] Trainer[1] register host memory...
[2022-12-08 19:23:29. 37895: W /samgraph/samgraph/common/dist/dist_engine.cc:701] Trainer[3] register host memory...
[2022-12-08 19:23:29. 65973: E /samgraph/samgraph/common/dist/dist_engine.cc:85] Running on V100
[2022-12-08 19:23:29. 66515: W /samgraph/samgraph/common/dist/dist_engine.cc:701] Trainer[4] register host memory...
[2022-12-08 19:23:29.135135: E /samgraph/samgraph/common/dist/dist_engine.cc:85] Running on V100
[2022-12-08 19:23:29.135247: W /samgraph/samgraph/common/dist/dist_engine.cc:701] Trainer[5] register host memory...
[2022-12-08 19:23:29.348552: E /samgraph/samgraph/common/dist/dist_engine.cc:85] Running on V100
[2022-12-08 19:23:29.349299: E /samgraph/samgraph/common/dist/dist_engine.cc:85] Running on V100
[2022-12-08 19:23:29.354637: W /samgraph/samgraph/common/cuda/cuda_device.cc:49] GPU[6] alloc cuda memory 931.40 MB
[2022-12-08 19:23:29.568534: W /samgraph/samgraph/common/cuda/cuda_device.cc:49] GPU[6] alloc cuda memory 12.87 GB
[2022-12-08 19:23:32.317270: W /samgraph/samgraph/common/cuda/cuda_device.cc:49] GPU[6] alloc cuda memory 32.00 MB
[2022-12-08 19:23:32.322783: E /samgraph/samgraph/common/dist/pre_sampler.cc:41] Dist Presampler making shuffler...
[2022-12-08 19:23:32.326834: E /samgraph/samgraph/common/dist/pre_sampler.cc:44] Dist Presampler making shuffler...Done
[2022-12-08 19:23:32.326884: W /samgraph/samgraph/common/cpu/cpu_device.cc:39] WORKER[0] alloc host memory 1.82 GB
[2022-12-08 19:23:33.125117: E /samgraph/samgraph/common/dist/pre_sampler.cc:85] Dist Presampler doing presample epoch 0
[2022-12-08 19:23:39.503671: E /samgraph/samgraph/common/dist/pre_sampler.cc:129] presample spend 2.18578 on sample, 0.175085 on copy, 4.00292 on count
[2022-12-08 19:23:39.503762: E /samgraph/samgraph/common/dist/pre_sampler.cc:85] Dist Presampler doing presample epoch 1
[2022-12-08 19:23:45.738962: E /samgraph/samgraph/common/dist/pre_sampler.cc:129] presample spend 4.39479 on sample, 0.349307 on copy, 7.84107 on count
[2022-12-08 19:23:45.739084: E /samgraph/samgraph/common/dist/pre_sampler.cc:136] max_num_inputs = 501274, min_num_inputs = 465202
[2022-12-08 19:23:46.226618: E /samgraph/samgraph/common/dist/pre_sampler.cc:148] presample spend 0.487438 on sort freq.
[2022-12-08 19:23:47. 89633: E /samgraph/samgraph/common/dist/dist_engine.cc:524] pre sample done, delete it
[[[[2022-12-08 19:23:47[[2022-12-08 19:23:472022-12-08 19:23:472022-12-08 19:23:472022-12-08 19:23:47.2022-12-08 19:23:47....149744.149741149760149747149751: 149748: : : : W: WWWW W    /samgraph/samgraph/common/dist/dist_engine.cc /samgraph/samgraph/common/dist/dist_engine.cc/samgraph/samgraph/common/dist/dist_engine.cc/samgraph/samgraph/common/dist/dist_engine.cc/samgraph/samgraph/common/dist/dist_engine.cc:/samgraph/samgraph/common/dist/dist_engine.cc::::711:711711711711] 711] ] ] ] Trainer[1] building cache...] Trainer[0] building cache...Trainer[5] building cache...Trainer[3] building cache...Trainer[4] building cache...
Trainer[2] building cache...




[[2022-12-08 19:23:47[2022-12-08 19:23:47.2022-12-08 19:23:47.151847.151854: 151863: [E: [[E2022-12-08 19:23:47 E2022-12-08 19:23:472022-12-08 19:23:47 ./samgraph/3rdparty/collcachelib/coll_cache_lib/run_config.cc ../samgraph/3rdparty/collcachelib/coll_cache_lib/run_config.cc151884:/samgraph/3rdparty/collcachelib/coll_cache_lib/run_config.cc151886151890:: 134:: : 134E] 134EE]  using concurrent impl MPS]   using concurrent impl MPS/samgraph/3rdparty/collcachelib/coll_cache_lib/run_config.cc
using concurrent impl MPS/samgraph/3rdparty/collcachelib/coll_cache_lib/run_config.cc/samgraph/3rdparty/collcachelib/coll_cache_lib/run_config.cc
:
::134134134] ] ] using concurrent impl MPSusing concurrent impl MPSusing concurrent impl MPS


[2022-12-08 19:23:47.155275: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:172] v100x8, slow pcie
[2022-12-08 19:23:47.155332: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:200] build asymm link desc with 6X Tesla V100-SXM2-32GB out of 8
[2022-12-08 19:23:47.155377: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:201] remote time is 8.68421
[2022-12-08 19:23:47.155405: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:202] cpu time is 97.0588
[2022-12-08 19:23:47.155419: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:172] v100x8, slow pcie
[[2022-12-08 19:23:472022-12-08 19:23:47..155464155473: : EE  /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc::172200] ] v100x8, slow pciebuild asymm link desc with 6X Tesla V100-SXM2-32GB out of 8
[
2022-12-08 19:23:47.155512: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:172[] 2022-12-08 19:23:47[v100x8, slow pcie.2022-12-08 19:23:47
155549.: [155553E2022-12-08 19:23:47:  [.E/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-08 19:23:47[155561 :.2022-12-08 19:23:47: /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc200155586.E:] : 155603 201build asymm link desc with 6X Tesla V100-SXM2-32GB out of 8E: /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 
 E:remote time is 8.68421/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [172
:/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[2022-12-08 19:23:47] 200:2022-12-08 19:23:47.v100x8, slow pcie] 172.155699
build asymm link desc with 6X Tesla V100-SXM2-32GB out of 8] 155718: 
v100x8, slow pcie: [E
E2022-12-08 19:23:47 [ ./samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-08 19:23:47[/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc155784:.2022-12-08 19:23:47:: 201155799.202E] : 155820]  remote time is 8.68421E: cpu time is 97.0588/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc
 E
:/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[ 200:2022-12-08 19:23:47/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc] 201.:build asymm link desc with 6X Tesla V100-SXM2-32GB out of 8] 155921200
remote time is 8.68421: ] 
Ebuild asymm link desc with 6X Tesla V100-SXM2-32GB out of 8 
/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc[[:2022-12-08 19:23:472022-12-08 19:23:47202.[.] 1560082022-12-08 19:23:47156009cpu time is 97.0588: .: 
E156034E :  /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccE/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc: :202/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc201] :] cpu time is 97.0588201remote time is 8.68421
] 
remote time is 8.68421
[2022-12-08 19:23:47[.2022-12-08 19:23:47156168.: 156172E:  E/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc :/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc202:] 202cpu time is 97.0588] 
cpu time is 97.0588
[2022-12-08 19:24:08.554932: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:118] solver created. now build & solve
[2022-12-08 19:24:08.650443: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:123] solver built. now solve
block 0 storage is 00001001
	access is	0	0	0	3	3	3	
block 1 storage is 00010010
	access is	1	1	1	4	4	4	
block 2 storage is 00100100
	access is	2	2	2	5	5	5	
block 3 storage is 00000000
	access is	6	6	6	6	6	6	
[2022-12-08 19:24:11. 69995: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:127] solver solved
[2022-12-08 19:24:11. 70078: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:195] 0 solved master
[2022-12-08 19:24:11. 70125: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:202] 0 solved
[2022-12-08 19:24:11. 70155: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:205] worker 0 thread 0 initing device 0
[2022-12-08 19:24:11. 71212: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1766] Building Coll Cache with ... num gpu device is 6
[2022-12-08 19:24:11.106603: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:202] 1 solved
[2022-12-08 19:24:11.106663: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:205] worker 1 thread 1 initing device 1
[2022-12-08 19:24:11.107241: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:202] 2 solved
[2022-12-08 19:24:11.107296: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:205] worker 2 thread 2 initing device 2
[2022-12-08 19:24:11.107577: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:202] 5 solved[
2022-12-08 19:24:11.107596[: 2022-12-08 19:24:11E. 107646/samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc: :E202 [] /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc2022-12-08 19:24:113 solved:.
205107682] [: worker 5 thread 5 initing device 52022-12-08 19:24:11E
. 107746/samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc: :E202 ] /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc4 solved:
205] worker 3 thread 3 initing device 3
[2022-12-08 19:24:11.107804: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:205] worker 4 thread 4 initing device 4
[2022-12-08 19:24:11.107979: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1766] Building Coll Cache with ... num gpu device is 6
[2022-12-08 19:24:11.110276: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1766] Building Coll Cache with ... num gpu device is 6
[2022-12-08 19:24:11.110344: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1766] Building Coll Cache with ... num gpu device is 6
[2022-12-08 19:24:11.110417: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1766] Building Coll Cache with ... num gpu device is 6
[2022-12-08 19:24:11.110480: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1766] Building Coll Cache with ... num gpu device is 6
[2022-12-08 19:24:11.256462: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1806] using empty feat=25
[2022-12-08 19:24:11.264921: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cpu/cpu_device.cc:39] WORKER[0] alloc host memory 65.20 MB
[2022-12-08 19:24:11.339844: W[ 2022-12-08 19:24:11/samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu.:3398841806: ] Wusing empty feat=25 
/samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1806] using empty feat=25
[2022-12-08 19:24:11.339962[: 2022-12-08 19:24:11W. 339994/samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu: :W1806 ] /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cuusing empty feat=25:
1806] using empty feat=25
[2022-12-08 19:24:11.340253: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1806] using empty feat=25
[[[[[[2022-12-08 19:24:152022-12-08 19:24:152022-12-08 19:24:152022-12-08 19:24:152022-12-08 19:24:152022-12-08 19:24:15......800569800563800572800564800564800565: : : : : : EEEEEE      /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu/samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu/samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu/samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu/samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu/samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu::::::187718771877187718771877] ] ] ] ] ] Device 5 init p2p of link 0Device 3 init p2p of link 4Device 0 init p2p of link 1Device 1 init p2p of link 0Device 4 init p2p of link 3Device 2 init p2p of link 1





[2022-12-08 19:24:15.818180: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1877] Device 5 init p2p of link 3
[2022-12-08 19:24:15.830437: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1877] Device 2 init p2p of link 0
[2022-12-08 19:24:15.831268: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1877] Device 0 init p2p of link 5
[2022-12-08 19:24:15.831380: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1877] Device 4 init p2p of link 2
[2022-12-08 19:24:15.831474: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1877] Device 3 init p2p of link 5
[2022-12-08 19:24:15.831628: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1877] Device 1 init p2p of link 2
[2022-12-08 19:24:15.835444: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1877] Device 5 init p2p of link 4
[2022-12-08 19:24:15.846004: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1877] Device 0 init p2p of link 2
[2022-12-08 19:24:15.848823: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1877] Device 4 init p2p of link 5
[2022-12-08 19:24:15.858545: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1877] Device 1 init p2p of link 3
[2022-12-08 19:24:15.858804: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1877] Device 3 init p2p of link 1
[2022-12-08 19:24:15.863304: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1877] Device 2 init p2p of link 4
[2022-12-08 19:24:15.867084: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1906] Asymm Coll cache (policy: clique_part) | local 17091234 / 244160499 nodes ( 7.00 %~7.00 %) | remote 34182468 / 244160499 nodes ( 14.00 %) | cpu 192886797 / 244160499 nodes ( 79.00 %) | 24.48 GB | 4.75905 secs 
[2022-12-08 19:24:15.867328: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1906] Asymm Coll cache (policy: clique_part) | local 17091234 / 244160499 nodes ( 7.00 %~7.00 %) | remote 34182468 / 244160499 nodes ( 14.00 %) | cpu 192886797 / 244160499 nodes ( 79.00 %) | 24.48 GB | 4.7568 secs 
[2022-12-08 19:24:15.873749: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1906] Asymm Coll cache (policy: clique_part) | local 17091234 / 244160499 nodes ( 7.00 %~7.00 %) | remote 34182468 / 244160499 nodes ( 14.00 %) | cpu 192886797 / 244160499 nodes ( 79.00 %) | 24.48 GB | 4.76343 secs 
[2022-12-08 19:24:15.875560: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1906] Asymm Coll cache (policy: clique_part) | local 17091234 / 244160499 nodes ( 7.00 %~7.00 %) | remote 34182468 / 244160499 nodes ( 14.00 %) | cpu 192886797 / 244160499 nodes ( 79.00 %) | 24.48 GB | 4.8043 secs 
[2022-12-08 19:24:15.876095: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1906] Asymm Coll cache (policy: clique_part) | local 17091234 / 244160499 nodes ( 7.00 %~7.00 %) | remote 34182468 / 244160499 nodes ( 14.00 %) | cpu 192886797 / 244160499 nodes ( 79.00 %) | 24.48 GB | 4.7657 secs 
[2022-12-08 19:24:15.876121: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2010] before create ctx, mem is 27.67 GB
[2022-12-08 19:24:15.925213: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1906] Asymm Coll cache (policy: clique_part) | local 17091234 / 244160499 nodes ( 7.00 %~7.00 %) | remote 34182468 / 244160499 nodes ( 14.00 %) | cpu 192886797 / 244160499 nodes ( 79.00 %) | 24.48 GB | 4.81475 secs 
[2022-12-08 19:24:17.386815: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2016] after create ctx, mem is 28.14 GB
[2022-12-08 19:24:17.386947: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2023] after create stream, mem is 28.14 GB
[2022-12-08 19:24:17.387320: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2010] before create ctx, mem is 28.14 GB
[2022-12-08 19:24:18.945881: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2016] after create ctx, mem is 28.60 GB
[2022-12-08 19:24:18.946010: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2023] after create stream, mem is 28.60 GB
[2022-12-08 19:24:18.946387: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2010] before create ctx, mem is 28.60 GB
[2022-12-08 19:24:20.592326: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2016] after create ctx, mem is 29.00 GB
[2022-12-08 19:24:20.592454: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2023] after create stream, mem is 29.00 GB
[2022-12-08 19:24:20.592776: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2010] before create ctx, mem is 29.00 GB
[2022-12-08 19:24:22.270095: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2016] after create ctx, mem is 29.64 GB
[2022-12-08 19:24:22.270216: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2023] after create stream, mem is 29.64 GB
[2022-12-08 19:24:22.270871: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2010] before create ctx, mem is 29.64 GB
[2022-12-08 19:24:23.958180: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2016] after create ctx, mem is 30.01 GB
[2022-12-08 19:24:23.958332: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2023] after create stream, mem is 30.01 GB
[2022-12-08 19:24:27.146469: W /samgraph/samgraph/common/cuda/cuda_device.cc:49] GPU[0] alloc cuda memory 801.38 MB
/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn("is_namedtuple is deprecated, please use the python checks instead")
Process Process-3:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 33, in decorated_function
    raise exception.__class__(trace)
dgl._ffi.base.DGLError: Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 21, in _queue_result
    res = func(*args, **kwargs)
  File "../../example/samgraph/multi_gpu/unsupervised/train_graphsage.py", line 349, in run_train
    batch_pred = model(blocks, batch_input)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1009, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 970, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "../../example/samgraph/multi_gpu/unsupervised/train_graphsage.py", line 47, in forward
    h = layer(block, h)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/nn/pytorch/conv/sageconv.py", line 235, in forward
    graph.update_all(msg_fn, fn.mean('m', 'neigh'))
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/heterograph.py", line 4895, in update_all
    ndata = core.message_passing(g, message_func, reduce_func, apply_node_func)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/core.py", line 357, in message_passing
    ndata = invoke_gspmm(g, mfunc, rfunc)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/core.py", line 332, in invoke_gspmm
    z = op(graph, x)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/ops/spmm.py", line 189, in func
    return gspmm(g, 'copy_lhs', reduce_op, x, None)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/ops/spmm.py", line 75, in gspmm
    ret = gspmm_internal(g._graph, op,
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/backend/pytorch/sparse.py", line 724, in gspmm
    return GSpMM.apply(gidx, op, reduce_op, lhs_data, rhs_data)
  File "/usr/local/lib/python3.8/dist-packages/torch/cuda/amp/autocast_mode.py", line 103, in decorate_fwd
    return fwd(*_cast(args, cast_inputs), **_cast(kwargs, cast_inputs))
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/backend/pytorch/sparse.py", line 106, in forward
    out, (argX, argY) = _gspmm(gidx, op, reduce_op, X, Y)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/sparse.py", line 228, in _gspmm
    _CAPI_DGLKernelSpMM(gidx, op, reduce_op,
  File "dgl/_ffi/_cython/./function.pxi", line 293, in dgl._ffi._cy3.core.FunctionBase.__call__
  File "dgl/_ffi/_cython/./function.pxi", line 239, in dgl._ffi._cy3.core.FuncCall
dgl._ffi.base.DGLError: [19:24:28] /dgl/src/runtime/cuda/cuda_device_api.cc:303: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading: CUDA: out of memory
Stack trace:
  [bt] (0) /usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/libdgl.so(+0x89f388) [0x7fe688d14388]
  [bt] (1) /usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/libdgl.so(dgl::runtime::CUDADeviceAPI::CopyDataFromTo(void const*, unsigned long, void*, unsigned long, unsigned long, DLContext, DLContext, DLDataType)+0x87) [0x7fe688d15c47]
  [bt] (2) /usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/libdgl.so(dgl::runtime::NDArray::CopyFromTo(DLTensor*, DLTensor*)+0x1c3) [0x7fe688b6e013]
  [bt] (3) /usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/libdgl.so(dgl::aten::COOSort(dgl::aten::COOMatrix, bool)+0x284) [0x7fe6888205a4]
  [bt] (4) /usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/libdgl.so(dgl::aten::CSRMatrix dgl::aten::impl::COOToCSR<(DLDeviceType)2, int>(dgl::aten::COOMatrix)+0xe4) [0x7fe688d43564]
  [bt] (5) /usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/libdgl.so(dgl::aten::COOToCSR(dgl::aten::COOMatrix)+0x443) [0x7fe6887d78f3]
  [bt] (6) /usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/libdgl.so(dgl::UnitGraph::GetInCSR(bool) const+0xf5) [0x7fe688ccf455]
  [bt] (7) /usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/libdgl.so(dgl::UnitGraph::GetCSCMatrix(unsigned long) const+0x2c) [0x7fe688ccfc8c]
  [bt] (8) /usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/libdgl.so(dgl::HeteroGraph::GetCSCMatrix(unsigned long) const+0x38) [0x7fe688bc37d8]



/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn("is_namedtuple is deprecated, please use the python checks instead")
Process Process-8:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 33, in decorated_function
    raise exception.__class__(trace)
dgl._ffi.base.DGLError: Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 21, in _queue_result
    res = func(*args, **kwargs)
  File "../../example/samgraph/multi_gpu/unsupervised/train_graphsage.py", line 349, in run_train
    batch_pred = model(blocks, batch_input)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1009, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 970, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "../../example/samgraph/multi_gpu/unsupervised/train_graphsage.py", line 47, in forward
    h = layer(block, h)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/nn/pytorch/conv/sageconv.py", line 235, in forward
    graph.update_all(msg_fn, fn.mean('m', 'neigh'))
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/heterograph.py", line 4895, in update_all
    ndata = core.message_passing(g, message_func, reduce_func, apply_node_func)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/core.py", line 357, in message_passing
    ndata = invoke_gspmm(g, mfunc, rfunc)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/core.py", line 332, in invoke_gspmm
    z = op(graph, x)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/ops/spmm.py", line 189, in func
    return gspmm(g, 'copy_lhs', reduce_op, x, None)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/ops/spmm.py", line 75, in gspmm
    ret = gspmm_internal(g._graph, op,
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/backend/pytorch/sparse.py", line 724, in gspmm
    return GSpMM.apply(gidx, op, reduce_op, lhs_data, rhs_data)
  File "/usr/local/lib/python3.8/dist-packages/torch/cuda/amp/autocast_mode.py", line 103, in decorate_fwd
    return fwd(*_cast(args, cast_inputs), **_cast(kwargs, cast_inputs))
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/backend/pytorch/sparse.py", line 106, in forward
    out, (argX, argY) = _gspmm(gidx, op, reduce_op, X, Y)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/sparse.py", line 228, in _gspmm
    _CAPI_DGLKernelSpMM(gidx, op, reduce_op,
  File "dgl/_ffi/_cython/./function.pxi", line 293, in dgl._ffi._cy3.core.FunctionBase.__call__
  File "dgl/_ffi/_cython/./function.pxi", line 239, in dgl._ffi._cy3.core.FuncCall
dgl._ffi.base.DGLError: [19:24:28] /dgl/src/runtime/cuda/cuda_device_api.cc:303: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading: CUDA: out of memory
Stack trace:
  [bt] (0) /usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/libdgl.so(+0x89f388) [0x7fe688d14388]
  [bt] (1) /usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/libdgl.so(dgl::runtime::CUDADeviceAPI::CopyDataFromTo(void const*, unsigned long, void*, unsigned long, unsigned long, DLContext, DLContext, DLDataType)+0x87) [0x7fe688d15c47]
  [bt] (2) /usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/libdgl.so(dgl::runtime::NDArray::CopyFromTo(DLTensor*, DLTensor*)+0x1c3) [0x7fe688b6e013]
  [bt] (3) /usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/libdgl.so(dgl::aten::COOSort(dgl::aten::COOMatrix, bool)+0x284) [0x7fe6888205a4]
  [bt] (4) /usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/libdgl.so(dgl::aten::CSRMatrix dgl::aten::impl::COOToCSR<(DLDeviceType)2, int>(dgl::aten::COOMatrix)+0xe4) [0x7fe688d43564]
  [bt] (5) /usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/libdgl.so(dgl::aten::COOToCSR(dgl::aten::COOMatrix)+0x443) [0x7fe6887d78f3]
  [bt] (6) /usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/libdgl.so(dgl::UnitGraph::GetInCSR(bool) const+0xf5) [0x7fe688ccf455]
  [bt] (7) /usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/libdgl.so(dgl::UnitGraph::GetCSCMatrix(unsigned long) const+0x2c) [0x7fe688ccfc8c]
  [bt] (8) /usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/libdgl.so(dgl::HeteroGraph::GetCSCMatrix(unsigned long) const+0x38) [0x7fe688bc37d8]



[5bb6a2c296f7:16513:0:16514] Caught signal 11 (Segmentation fault: address not mapped to object at address 0xb8)
==== backtrace (tid:  16514) ====
 0 0x0000000000014420 __funlockfile()  ???:0
 1 0x00000000005b0111 _PyTuple_Resize()  ???:0
 2 0x00000000002db927 THPFunction_apply()  python_function.cpp:0
 3 0x00000000005f69ca PyCFunction_Call()  ???:0
 4 0x00000000005f74f6 _PyObject_MakeTpCall()  ???:0
 5 0x0000000000571164 _PyEval_EvalFrameDefault()  ???:0
 6 0x00000000005f6cd6 _PyFunction_Vectorcall()  ???:0
 7 0x000000000056bacd _PyEval_EvalFrameDefault()  ???:0
 8 0x00000000005f6cd6 _PyFunction_Vectorcall()  ???:0
 9 0x000000000056bacd _PyEval_EvalFrameDefault()  ???:0
10 0x0000000000569dba _PyEval_EvalCodeWithName()  ???:0
11 0x00000000005f6eb3 _PyFunction_Vectorcall()  ???:0
12 0x000000000056bacd _PyEval_EvalFrameDefault()  ???:0
13 0x0000000000569dba _PyEval_EvalCodeWithName()  ???:0
14 0x00000000005f6eb3 _PyFunction_Vectorcall()  ???:0
15 0x000000000056bacd _PyEval_EvalFrameDefault()  ???:0
16 0x00000000005f6cd6 _PyFunction_Vectorcall()  ???:0
17 0x0000000000570b26 _PyEval_EvalFrameDefault()  ???:0
18 0x0000000000569dba _PyEval_EvalCodeWithName()  ???:0
19 0x00000000005f6eb3 _PyFunction_Vectorcall()  ???:0
20 0x000000000056bbfa _PyEval_EvalFrameDefault()  ???:0
21 0x0000000000569dba _PyEval_EvalCodeWithName()  ???:0
22 0x00000000005f6eb3 _PyFunction_Vectorcall()  ???:0
23 0x000000000050bc2c PyMethod_New()  ???:0
24 0x00000000005f6082 PyObject_Call()  ???:0
25 0x000000000056d2d5 _PyEval_EvalFrameDefault()  ???:0
26 0x0000000000569dba _PyEval_EvalCodeWithName()  ???:0
27 0x00000000005f6eb3 _PyFunction_Vectorcall()  ???:0
28 0x000000000059d81e PyUnicode_New()  ???:0
29 0x00000000005f74f6 _PyObject_MakeTpCall()  ???:0
30 0x0000000000570d55 _PyEval_EvalFrameDefault()  ???:0
31 0x00000000005f6cd6 _PyFunction_Vectorcall()  ???:0
32 0x000000000050bc2c PyMethod_New()  ???:0
33 0x00000000005f6082 PyObject_Call()  ???:0
34 0x000000000056d2d5 _PyEval_EvalFrameDefault()  ???:0
35 0x0000000000569dba _PyEval_EvalCodeWithName()  ???:0
36 0x00000000005f6eb3 _PyFunction_Vectorcall()  ???:0
37 0x000000000059d89f PyUnicode_New()  ???:0
38 0x00000000005f627e PyObject_Call()  ???:0
39 0x000000000056d2d5 _PyEval_EvalFrameDefault()  ???:0
40 0x0000000000569dba _PyEval_EvalCodeWithName()  ???:0
41 0x00000000005f6eb3 _PyFunction_Vectorcall()  ???:0
42 0x000000000050bc2c PyMethod_New()  ???:0
43 0x00000000005f6082 PyObject_Call()  ???:0
44 0x000000000056d2d5 _PyEval_EvalFrameDefault()  ???:0
45 0x0000000000569dba _PyEval_EvalCodeWithName()  ???:0
46 0x00000000005f6eb3 _PyFunction_Vectorcall()  ???:0
47 0x000000000050bc2c PyMethod_New()  ???:0
48 0x00000000005f6082 PyObject_Call()  ???:0
49 0x000000000056d2d5 _PyEval_EvalFrameDefault()  ???:0
50 0x0000000000569dba _PyEval_EvalCodeWithName()  ???:0
51 0x00000000005f6eb3 _PyFunction_Vectorcall()  ???:0
52 0x000000000059d81e PyUnicode_New()  ???:0
53 0x00000000005f74f6 _PyObject_MakeTpCall()  ???:0
54 0x0000000000570d55 _PyEval_EvalFrameDefault()  ???:0
55 0x00000000005f6cd6 _PyFunction_Vectorcall()  ???:0
56 0x00000000005f6082 PyObject_Call()  ???:0
57 0x000000000056d2d5 _PyEval_EvalFrameDefault()  ???:0
=================================
/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn("is_namedtuple is deprecated, please use the python checks instead")
Process Process-4:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 33, in decorated_function
    raise exception.__class__(trace)
RuntimeError: Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 21, in _queue_result
    res = func(*args, **kwargs)
  File "../../example/samgraph/multi_gpu/unsupervised/train_graphsage.py", line 349, in run_train
    batch_pred = model(blocks, batch_input)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1009, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 970, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "../../example/samgraph/multi_gpu/unsupervised/train_graphsage.py", line 47, in forward
    h = layer(block, h)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/nn/pytorch/conv/sageconv.py", line 274, in forward
    rst = rst + self.bias
RuntimeError: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 1; 31.75 GiB total capacity; 298.76 MiB already allocated; 27.88 MiB free; 322.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn("is_namedtuple is deprecated, please use the python checks instead")
Process Process-5:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 33, in decorated_function
    raise exception.__class__(trace)
RuntimeError: Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 21, in _queue_result
    res = func(*args, **kwargs)
  File "../../example/samgraph/multi_gpu/unsupervised/train_graphsage.py", line 352, in run_train
    scaler.scale(loss).backward()
  File "/usr/local/lib/python3.8/dist-packages/torch/_tensor.py", line 402, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py", line 191, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`

[2022-12-08 19:24:38.774063: E /samgraph/samgraph/common/operation.cc:567] detect a terminated child 16519, status is 1
