[2022-12-05 18:59:58.671808: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:92] mmap allocating space 13.46 GB
[2022-12-05 18:59:58.671913: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:97] mmap allocating space 13.46 GB done
[2022-12-05 19:00:04. 76755: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:92] mmap allocating space 62.57 GB
[2022-12-05 19:00:04. 76880: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:97] mmap allocating space 62.57 GB done
[2022-12-05 19:00:04. 80656: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:92] mmap allocating space 8.62 GB
[2022-12-05 19:00:04. 80705: W /samgraph/samgraph/common/cpu/mmap_cpu_device.cc:97] mmap allocating space 8.62 GB done
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
[2022-12-05 19:00:04.130782: W /samgraph/samgraph/common/dist/dist_engine.cc:662] Trainer[1] initializing...
[2022-12-05 19:00:04.330211: W /samgraph/samgraph/common/dist/dist_engine.cc:662] Trainer[0] initializing...
[2022-12-05 19:00:04.332030: W /samgraph/samgraph/common/dist/dist_engine.cc:662] Trainer[2] initializing...
[2022-12-05 19:00:05.647818: W /samgraph/samgraph/common/dist/dist_engine.cc:666] Trainer[1] pin memory queue...
[[2022-12-05 19:00:05.2022-12-05 19:00:05740317.: 740305W:  W/samgraph/samgraph/common/dist/dist_engine.cc :/samgraph/samgraph/common/dist/dist_engine.cc666:] 666Trainer[0] pin memory queue...] 
Trainer[2] pin memory queue...
[2022-12-05 19:00:08[.2022-12-05 19:00:08477995.: [478024E2022-12-05 19:00:08:  .E/samgraph/samgraph/common/dist/dist_engine.cc478051 :: /samgraph/samgraph/common/dist/dist_engine.cc85E:]  85Running on V100/samgraph/samgraph/common/dist/dist_engine.cc] 
:Running on V10085
] Running on V100
[2022-12-05 19:00:08.478173: W /samgraph/samgraph/common/dist/dist_engine.cc:701] [Trainer[2] register host memory...2022-12-05 19:00:08
.[4781922022-12-05 19:00:08: .W478203 : /samgraph/samgraph/common/dist/dist_engine.ccW: 701/samgraph/samgraph/common/dist/dist_engine.cc] :Trainer[1] register host memory...701
] Trainer[0] register host memory...
[2022-12-05 19:00:08.707423: E /samgraph/samgraph/common/dist/dist_engine.cc:85] Running on V100
[2022-12-05 19:00:08.710527: W /samgraph/samgraph/common/cuda/cuda_device.cc:49] GPU[3] alloc cuda memory 250.28 MB
[2022-12-05 19:00:08.781548: W /samgraph/samgraph/common/cuda/cuda_device.cc:49] GPU[3] alloc cuda memory 13.46 GB
[2022-12-05 19:00:11.908628: W /samgraph/samgraph/common/cuda/cuda_device.cc:49] GPU[3] alloc cuda memory 128.00 MB
[2022-12-05 19:00:11.912067: E /samgraph/samgraph/common/dist/pre_sampler.cc:41] Dist Presampler making shuffler...
[2022-12-05 19:00:11.915115: E /samgraph/samgraph/common/dist/pre_sampler.cc:44] Dist Presampler making shuffler...Done
[2022-12-05 19:00:11.915160: W /samgraph/samgraph/common/cpu/cpu_device.cc:39] WORKER[0] alloc host memory 500.55 MB
[2022-12-05 19:00:12. 88459: E /samgraph/samgraph/common/dist/pre_sampler.cc:85] Dist Presampler doing presample epoch 0
[2022-12-05 19:00:13.222995: E /samgraph/samgraph/common/dist/pre_sampler.cc:129] presample spend 0.658546 on sample, 0.0487055 on copy, 0.425983 on count
[2022-12-05 19:00:13.223080: E /samgraph/samgraph/common/dist/pre_sampler.cc:85] Dist Presampler doing presample epoch 1
[2022-12-05 19:00:14.351987: E /samgraph/samgraph/common/dist/pre_sampler.cc:129] presample spend 1.31406 on sample, 0.0952646 on copy, 0.851565 on count
[2022-12-05 19:00:14.352096: E /samgraph/samgraph/common/dist/pre_sampler.cc:136] max_num_inputs = 1211340, min_num_inputs = 1164480
[2022-12-05 19:00:14.556117: E /samgraph/samgraph/common/dist/pre_sampler.cc:148] presample spend 0.203919 on sort freq.
[2022-12-05 19:00:14.755561: E /samgraph/samgraph/common/dist/dist_engine.cc:524] pre sample done, delete it
[[[2022-12-05 19:00:142022-12-05 19:00:142022-12-05 19:00:14...770771770783770781: : : WWW   /samgraph/samgraph/common/dist/dist_engine.cc/samgraph/samgraph/common/dist/dist_engine.cc/samgraph/samgraph/common/dist/dist_engine.cc:::711711711] ] ] Trainer[1] building cache...Trainer[0] building cache...Trainer[2] building cache...


[2022-12-05 19:00:14.771566: E /samgraph/3rdparty/collcachelib/coll_cache_lib/run_config.cc:134] using concurrent impl MPS
[2022-12-05 19:00:14[.2022-12-05 19:00:14771696.: 771709E:  E/samgraph/3rdparty/collcachelib/coll_cache_lib/run_config.cc :/samgraph/3rdparty/collcachelib/coll_cache_lib/run_config.cc134:] 134using concurrent impl MPS] 
using concurrent impl MPS
[2022-12-05 19:00:14.771887: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] build symm link desc with 3X Tesla V100-SXM2-16GB out of 4
[2022-12-05 19:00:14.771930: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:197] remote time is 8.68421
[2022-12-05 19:00:14.771958: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:198] cpu time is 30
[2022-12-05 19:00:14.772805: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[1962022-12-05 19:00:14] .772845build symm link desc with 3X Tesla V100-SXM2-16GB out of 4: 
E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] [build symm link desc with 3X Tesla V100-SXM2-16GB out of 42022-12-05 19:00:14
.772924: E [/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-05 19:00:14:.197772968] : remote time is 8.68421E
 /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:197[] 2022-12-05 19:00:14remote time is 8.68421.
773036: E [/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-05 19:00:14:.198773079] : cpu time is 30E
 /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:198] cpu time is 30
[2022-12-05 19:00:35.605333: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:115] solver created. now build & solve
[2022-12-05 19:00:35.633294: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:120] solver built. now solve
[2022-12-05 19:00:35.633350: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:895] num_cached_nodes = 10497338
[2022-12-05 19:00:35.999460: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:124] solver solved
[2022-12-05 19:00:35.999554: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:187] 0 solved master
[2022-12-05 19:00:35.999595: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:194] 0 solved
[2022-12-05 19:00:35.999623: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:197] worker 0 thread 0 initing device 0
[2022-12-05 19:00:36. 11391: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:194] 2 solved
[2022-12-05 19:00:36. 11448: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:197] worker 2 thread 2 initing device 2[
2022-12-05 19:00:36. 11458: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:194] 1 solved
[2022-12-05 19:00:36. 11511: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:197] worker 1 thread 1 initing device 1
[2022-12-05 19:00:36.988216: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1740] Collaborative GPU cache (policy: rep_cache) | local 10497338 / 65608366 nodes ( 16.00 %~16.00 %) | remote 0 / 65608366 nodes ( 0.00 %) | cpu 55111028 / 65608366 nodes ( 84.00 %) | 10.01 GB | 0.975565 secs 
[2022-12-05 19:00:36.988605: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1740] Collaborative GPU cache (policy: rep_cache) | local 10497338 / 65608366 nodes ( 16.00 %~16.00 %) | remote 0 / 65608366 nodes ( 0.00 %) | cpu 55111028 / 65608366 nodes ( 84.00 %) | 10.01 GB | 0.975919 secs 
[2022-12-05 19:00:36.988918: E /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:1740] Collaborative GPU cache (policy: rep_cache) | local 10497338 / 65608366 nodes ( 16.00 %~16.00 %) | remote 0 / 65608366 nodes ( 0.00 %) | cpu 55111028 / 65608366 nodes ( 84.00 %) | 10.01 GB | 0.988128 secs 
[2022-12-05 19:00:36.989192: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2004] before create ctx, mem is 11.34 GB
[2022-12-05 19:00:38.309396: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2010] after create ctx, mem is 11.83 GB
[2022-12-05 19:00:38.309534: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2017] after create stream, mem is 11.83 GB
[2022-12-05 19:00:38.309986: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2004] before create ctx, mem is 11.83 GB
[2022-12-05 19:00:39.520744: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2010] after create ctx, mem is 12.32 GB
[2022-12-05 19:00:39.521596: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2017] after create stream, mem is 12.32 GB
[2022-12-05 19:00:39.521856: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2004] before create ctx, mem is 12.32 GB
[2022-12-05 19:00:40.809372: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2010] after create ctx, mem is 12.96 GB
[2022-12-05 19:00:40.809515: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2017] after create stream, mem is 12.96 GB
[2022-12-05 19:00:40.809857: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2004] before create ctx, mem is 12.96 GB
[2022-12-05 19:00:42.152962: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2010] after create ctx, mem is 13.33 GB
[2022-12-05 19:00:42.153093: W /samgraph/3rdparty/collcachelib/coll_cache_lib/cache_context.cu:2017] after create stream, mem is 13.33 GB
[[2022-12-05 19:00:42.2022-12-05 19:00:42177446[.: 177447E2022-12-05 19:00:42:  .E/samgraph/3rdparty/collcachelib/coll_cache_lib/run_config.cc177448 :: /samgraph/3rdparty/collcachelib/coll_cache_lib/run_config.cc134E:]  134using concurrent impl MPS/samgraph/3rdparty/collcachelib/coll_cache_lib/run_config.cc] 
:using concurrent impl MPS134
] using concurrent impl MPS
[2022-12-05 19:00:42.178336: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:196] build symm link desc with 3X Tesla V100-SXM2-16GB out of 4[
2022-12-05 19:00:42.178357: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:[1962022-12-05 19:00:42[] .2022-12-05 19:00:42build symm link desc with 3X Tesla V100-SXM2-16GB out of 4178380.
: 178380E:  E/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc [:/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-05 19:00:42197:.] 196178416remote time is 8.68421] : 
build symm link desc with 3X Tesla V100-SXM2-16GB out of 4E
 [/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc2022-12-05 19:00:42:.197178453[] : 2022-12-05 19:00:42remote time is 8.68421
.178464E[:  2022-12-05 19:00:42E/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc. :178502/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc198: :] E197cpu time is 30 ] 
/samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.ccremote time is 8.68421:
198] cpu time is 30
[2022-12-05 19:00:42.178561: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/asymm_link_desc.cc:198] cpu time is 30
[2022-12-05 19:00:42.347665: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:115] solver created. now build & solve
[2022-12-05 19:00:42.362637: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:120] solver built. now solve
[2022-12-05 19:00:42.362686: E /samgraph/3rdparty/collcachelib/coll_cache_lib/coll_cache/optimal_solver_class.cc:895] num_cached_nodes = 10497338
[2022-12-05 19:00:42.727330: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:124] solver solved
[2022-12-05 19:00:42.727401: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:187] 0 solved master
[2022-12-05 19:00:42.727429: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:194] 0 solved
[2022-12-05 19:00:42.727456: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:197] worker 0 thread 0 initing device 0
[2022-12-05 19:00:42.739159: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:194] 2 solved
[2022-12-05 19:00:42.739221: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:197] worker 2 thread 2 initing device 2
[2022-12-05 19:00:42.742590: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:194] 1 solved
[2022-12-05 19:00:42.742644: E /samgraph/3rdparty/collcachelib/coll_cache_lib/facade.cc:197] worker 1 thread 1 initing device 1
[2022-12-05 19:00:44.232261: W /samgraph/samgraph/common/cuda/cuda_device.cc:49] GPU[0] alloc cuda memory 1.23 GB
[2022-12-05 19:00:44.232947: W /samgraph/samgraph/common/cuda/cuda_device.cc:49] GPU[0] alloc cuda memory 12.31 MB
[2022-12-05 19:00:44.233506: W /samgraph/samgraph/common/cuda/cuda_device.cc:49] GPU[0] alloc cuda memory 12.31 MB
/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn("is_namedtuple is deprecated, please use the python checks instead")
Process Process-2:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 33, in decorated_function
    raise exception.__class__(trace)
RuntimeError: Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 21, in _queue_result
    res = func(*args, **kwargs)
  File "../../example/samgraph/multi_gpu/train_graphsage.py", line 320, in run_train
    batch_pred = model(blocks, batch_input)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1009, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 970, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "../../example/samgraph/multi_gpu/train_graphsage.py", line 48, in forward
    h = layer(block, h)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/nn/pytorch/conv/sageconv.py", line 238, in forward
    h_neigh = self.fc_neigh(h_neigh)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`

/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn("is_namedtuple is deprecated, please use the python checks instead")
Process Process-4:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 33, in decorated_function
    raise exception.__class__(trace)
RuntimeError: Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 21, in _queue_result
    res = func(*args, **kwargs)
  File "../../example/samgraph/multi_gpu/train_graphsage.py", line 320, in run_train
    batch_pred = model(blocks, batch_input)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1009, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 970, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "../../example/samgraph/multi_gpu/train_graphsage.py", line 48, in forward
    h = layer(block, h)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/nn/pytorch/conv/sageconv.py", line 238, in forward
    h_neigh = self.fc_neigh(h_neigh)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`

/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn("is_namedtuple is deprecated, please use the python checks instead")
Process Process-3:
Traceback (most recent call last):
  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 33, in decorated_function
    raise exception.__class__(trace)
RuntimeError: Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/multiprocessing/pytorch.py", line 21, in _queue_result
    res = func(*args, **kwargs)
  File "../../example/samgraph/multi_gpu/train_graphsage.py", line 320, in run_train
    batch_pred = model(blocks, batch_input)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 1009, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/distributed.py", line 970, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "../../example/samgraph/multi_gpu/train_graphsage.py", line 48, in forward
    h = layer(block, h)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/dgl-0.9.1-py3.8-linux-x86_64.egg/dgl/nn/pytorch/conv/sageconv.py", line 238, in forward
    h_neigh = self.fc_neigh(h_neigh)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`

[2022-12-05 19:00:47.591469: E /samgraph/samgraph/common/operation.cc:567] detect a terminated child 152939, status is 1
